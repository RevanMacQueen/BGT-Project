{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils \n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Level_K_Model:\n",
    "    def __init__(self, K, h):\n",
    "        self.K = K\n",
    "        self.payoffs = np.zeros([2, 2, 2])\n",
    "        self.payoffs[0, :, :] = np.array([[0, -1],\n",
    "                                          [1, -1.2]])\n",
    "        self.payoffs[1, :, :] = np.array([[0, 1],\n",
    "                                         [-1, -1.2]])\n",
    "        self.alphas = None\n",
    "        self.h = h # the history length\n",
    "\n",
    "\n",
    "    def log_likelihood_independent(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood. This assumes a players level may change as the game goes on\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        \n",
    "        alphas = params[0:-2]\n",
    "        lambda_ = params[-2] \n",
    "        kappa  = params[-1] \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                traj = play[player]\n",
    "                other_traj  = play[1-player] # trajectory of other player\n",
    "                \n",
    "                pred_s = self.predict_traj(traj, other_traj, self.K, lambda_, kappa, overall=True, alphas=alphas)  \n",
    "               \n",
    "                L = traj.shape[1] #length of trajectory\n",
    "                for l in range(L):\n",
    "                    idx = np.where(traj[:, l])\n",
    "                    ll += np.log(pred_s[:, l][idx][0])\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def log_likelihood(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood.\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        alphas = params[0:-2]\n",
    "        lambda_ = params[-2] \n",
    "        kappa  = params[-1] \n",
    "       \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                sum = 0\n",
    "                for i, k in enumerate(range(self.K+1)): # condition on a specific value of k for a player\n",
    "                    alpha_k = alphas[i]\n",
    "    \n",
    "                    traj = play[player]\n",
    "                    other_traj  = play[1-player] # trajectory of other player\n",
    "                    pred_s = self.predict_traj(traj, other_traj, k, lambda_, kappa)\n",
    "                    \n",
    "                    prob = p_traj(traj, pred_s) # probability of that trajectory\n",
    "                    \n",
    "                    sum += alpha_k * prob # this could be 0, so small epsilon added\n",
    "\n",
    "                ll += np.log(sum)\n",
    "                #print(np.log(sum))\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        params = np.zeros(self.K+3) # +2 since level 0 and the lambda parameter, kappa parameter\n",
    "        params[0] = 1 # inital guess is that all players are level 0\n",
    "        params[-2] = 5 # intial guess for lambda\n",
    "        params[-1] = 0.5 # intial guess for kappa\n",
    "\n",
    "        const_arr = np.ones(self.K+3)\n",
    "        const_arr[-1] = 0\n",
    "        const_arr[-2] = 0\n",
    "        #print(const_arr)\n",
    "        constraint = LinearConstraint(const_arr, lb=1, ub=1)\n",
    "        bnds = [(0, 1) for x in range(self.K+1)]\n",
    "        bnds.append((0, 1000)) \n",
    "        bnds.append((0, 1)) \n",
    "        #print(bnds)\n",
    "\n",
    "        result = minimize(\n",
    "            self.log_likelihood_independent ,\n",
    "            params, \n",
    "            args=(dataset),\n",
    "            bounds=bnds, \n",
    "            constraints=constraint) \n",
    "        print(result)\n",
    "\n",
    "        assert result.status == 0 # make sure the optimization was successful\n",
    "        self.alphas = result.x[0:-1]\n",
    "        self.lambda_ = result.x[-1]\n",
    "        ll = result.fun\n",
    "        return ll\n",
    "\n",
    "\n",
    "    def predict_traj(self, traj, other_traj, K, lambda_, kappa, overall=False, alphas=None):\n",
    "        '''\n",
    "        Returns straetgy predictions against other player\n",
    "        \n",
    "        '''\n",
    "        def get_weights(h, kappa):\n",
    "            if h == 1:\n",
    "                return np.ones(1)\n",
    "            \n",
    "            l = [1-kappa]\n",
    "            for i in range(1, h):\n",
    "                l.append(l[i-1]*kappa)\n",
    "\n",
    "            weight_left = 1- np.sum(l)\n",
    "            l[0] = l[0] + weight_left\n",
    "            \n",
    "            l = np.flip(np.array(l))\n",
    "\n",
    "            return l \n",
    "\n",
    "        L = other_traj.shape[1]\n",
    "        pred_i = np.zeros((2, self.K+1, L)) # level-k prediction for each stage game for i\n",
    "        pred_other = np.zeros((2, self.K+1, L)) # level-k prediction for each stage game for other player\n",
    "\n",
    "        for l in range(L):\n",
    "            # first determine level 0 strategy \n",
    "            start_hist_idx = 0 #max(l-self.h, 0)\n",
    "            end_hist_idx = l\n",
    "\n",
    "            if l == 0: # there is no history, so level-0 strategies are uniform\n",
    "                lvl_0_s_i = np.ones((2)) / 2\n",
    "                lvl_0_s_other = np.ones((2)) / 2\n",
    "            else:\n",
    "                hist_i = traj[:, start_hist_idx:end_hist_idx] # limited history of i's actions\n",
    "                hist_other = other_traj[:, start_hist_idx:end_hist_idx] # limited history of -i's actions\n",
    "\n",
    "                w = get_weights(hist_i.shape[1], kappa)\n",
    "                lvl_0_s_i =  0.99*np.dot(hist_i, w)+ 0.01 * np.ones((2))/2\n",
    "                lvl_0_s_other=  0.99*np.dot(hist_other, w)+ 0.01 * np.ones((2))/2\n",
    "\n",
    "\n",
    "            # These become the level 0 strategies\n",
    "            pred_i[:, 0, l] = lvl_0_s_i\n",
    "            pred_other[:, 0, l] = lvl_0_s_other\n",
    "\n",
    "            # Now, for higher levels:\n",
    "            for k in range(1, K+1):\n",
    "                pred_i[:, k, l] = 0.999*self.compute_BR(pred_other[:, k-1, l], lambda_) + 0.001 * np.ones((2))/2\n",
    "                pred_other[:, k, l] =  0.999*self.compute_BR(pred_i[:, k-1, l], lambda_)+ 0.001 * np.ones((2))/2\n",
    " \n",
    "\n",
    "        if overall: # if return the overall prediction\n",
    "            assert alphas is not None\n",
    "            pred_i_ = np.zeros((2, L))\n",
    "\n",
    "            for l in range(L):\n",
    "                pred_i[:, 0, l] = np.ones((2)) / 2\n",
    "                pred_i_[:, l] = np.dot(pred_i[:, :, l], alphas)\n",
    "            pred_i = pred_i_\n",
    "     \n",
    "        else:\n",
    "            pred_i = np.squeeze(pred_i[:, K, :]) # only return the level K predictions\n",
    "\n",
    "\n",
    "        # if K == 0:\n",
    "        #     return np.ones((2, L)) / (np.ones(L)*2)\n",
    "        # else:\n",
    "        return pred_i   \n",
    "\n",
    "\n",
    "    def compute_BR(self,  s_other, lambda_):\n",
    "        '''\n",
    "        Computes a best response\n",
    "\n",
    "        Parameters:\n",
    "            s_other : (np.Array) strategy of other player\n",
    "\n",
    "        NOTE: this ONLY works with symetric payoffs and 2 actions\n",
    "        '''\n",
    "    \n",
    "        #get EU of action 0\n",
    "    \n",
    "        s = [np.array([1, 0]), s_other]\n",
    "        eu_0 = expected_utility(s, self.payoffs[0])\n",
    "        #print('EU 0: {}'.format( eu_0))\n",
    "\n",
    "        # get EU of action 1\n",
    "        s = [np.array([0, 1]), s_other]\n",
    "        eu_1 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        #print('EU 1: {}'.format( eu_1))\n",
    "\n",
    "        # return action with greater EU\n",
    "        return softmax(np.array([eu_0, eu_1])*lambda_)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
