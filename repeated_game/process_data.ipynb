{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils \n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('repeated_data.csv', delimiter=',', dtype=str)\n",
    "data = np.delete(data, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "\n",
    "# turn each game into entry in this array\n",
    "\n",
    "def remove_blank(l):\n",
    "    return list(filter(lambda a: a != '', l))\n",
    "\n",
    "def replace_letters(l):\n",
    "   return [[1,0] if x=='c' else [0, 1] for x in l]\n",
    "\n",
    "for i in range(0, data.shape[0], 2):\n",
    "    p1 = data[i]\n",
    "    p2 = data[i+1]\n",
    "\n",
    "    game = int(p1[0])\n",
    "    player1 = p1[2]\n",
    "    player2 = p2[2]\n",
    "    p1_actions = np.array(replace_letters(remove_blank(p1[3:]))).T\n",
    "    p2_actions =  np.array(replace_letters(remove_blank(p2[3:]))).T\n",
    "   \n",
    "    actions =  [p1_actions, p2_actions]\n",
    "\n",
    "    new_data.append([game, player1, player2, actions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new_data)\n",
    "df.columns = ['Game', 'Player_1', 'Player_2', 'Actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game</th>\n",
       "      <th>Player_1</th>\n",
       "      <th>Player_2</th>\n",
       "      <th>Actions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Faisal</td>\n",
       "      <td>Bharathvaj</td>\n",
       "      <td>[[[0, 1, 1, 1, 0], [1, 0, 0, 0, 1]], [[1, 0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yuan</td>\n",
       "      <td>Dagmar</td>\n",
       "      <td>[[[1, 0, 0], [0, 1, 1]], [[1, 1, 0], [0, 0, 1]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ehsan</td>\n",
       "      <td>Mohammend</td>\n",
       "      <td>[[[1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tao</td>\n",
       "      <td>Rohini</td>\n",
       "      <td>[[[0, 1, 0, 1, 1], [1, 0, 1, 0, 0]], [[1, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Anita</td>\n",
       "      <td>Greg</td>\n",
       "      <td>[[[1, 1, 0, 1, 1, 1, 0], [0, 0, 1, 0, 0, 0, 1]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Game Player_1    Player_2  \\\n",
       "0     0   Faisal  Bharathvaj   \n",
       "1     1    Yuan       Dagmar   \n",
       "2     2    Ehsan   Mohammend   \n",
       "3     3      Tao      Rohini   \n",
       "4     4    Anita        Greg   \n",
       "\n",
       "                                             Actions  \n",
       "0  [[[0, 1, 1, 1, 0], [1, 0, 0, 0, 1]], [[1, 0, 1...  \n",
       "1   [[[1, 0, 0], [0, 1, 1]], [[1, 1, 0], [0, 0, 1]]]  \n",
       "2  [[[1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0,...  \n",
       "3  [[[0, 1, 0, 1, 1], [1, 0, 1, 0, 0]], [[1, 0, 0...  \n",
       "4  [[[1, 1, 0, 1, 1, 1, 0], [0, 0, 1, 0, 0, 0, 1]...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2 0.4]\n",
      " [0.2 0.2]]\n",
      "[[0.33333333 0.        ]\n",
      " [0.33333333 0.33333333]]\n",
      "[[0.5        0.16666667]\n",
      " [0.33333333 0.        ]]\n",
      "[[0.4 0.2]\n",
      " [0.2 0.2]]\n",
      "[[0.57142857 0.14285714]\n",
      " [0.28571429 0.        ]]\n",
      "[[0.5 0.5]\n",
      " [0.  0. ]]\n",
      "[[0.07142857 0.14285714]\n",
      " [0.21428571 0.57142857]]\n",
      "[[0.5 0. ]\n",
      " [0.5 0. ]]\n",
      "[[0.2 0.4]\n",
      " [0.4 0. ]]\n",
      "[[0. 0.]\n",
      " [0. 1.]]\n",
      "[[0.  0. ]\n",
      " [0.5 0.5]]\n",
      "[[0.5 0. ]\n",
      " [0.5 0. ]]\n",
      "[[0.55555556 0.22222222]\n",
      " [0.11111111 0.11111111]]\n",
      "[[0.09090909 0.18181818]\n",
      " [0.36363636 0.36363636]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    actions = row.Actions\n",
    "    plays = np.zeros((2,2))\n",
    "    for round in range( actions[0].shape[1]): # iterate over length of each game \n",
    "        s1 = actions[0][:, round]\n",
    "        s2 = actions[1][:, round]\n",
    "        p_outcomes = utils.p_outcomes([s1, s2])\n",
    "        plays += p_outcomes\n",
    "      \n",
    "    # frequency of each\n",
    "    print(plays/np.sum(plays))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis_labels = ['Cautious', 'Free']\n",
    "y_axis_labels = ['Cautious', 'Free']\n",
    "sns.set(font_scale=2) \n",
    "sns.heatmap(plays/np.sum(plays), annot=True, cbar=False, xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_traj(a, s):\n",
    "    '''\n",
    "    Returns the probability of a sequence of actions\n",
    "    a being sampled from strategy s.\n",
    "\n",
    "    Paramters:\n",
    "        a : (np.Array) 2 x n array of actions chosen\n",
    "        s : (np.Array) 2 x n array of strategies \n",
    "    '''\n",
    "\n",
    "    assert a.shape == s.shape\n",
    "    L = a.shape[1]\n",
    "    prod = 1\n",
    "    for l in range(L):\n",
    "        idx = np.where(a[:, l])\n",
    "        prod *= s[:, l][idx][0]\n",
    "\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(df.Actions)\n",
    "traj = data[0][0][:, 0:3]\n",
    "s = np.array([[0.9, 0.1], [0.6, 0.4], [.3, .7] ]).T\n",
    "assert p_traj(traj, s) == 0.018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_outcomes(s):\n",
    "    '''\n",
    "    returns a probability distribution over outcomes\n",
    "    '''\n",
    "    p = s[0]\n",
    "    for s_i in s[1:]:\n",
    "        p =  np.multiply.outer(p, s_i)\n",
    "    return p\n",
    "\n",
    "def expected_utility(s, payoffs):\n",
    "    '''\n",
    "    Gets the expected utilitiy for a player under strategy profile s\n",
    "    \n",
    "    Parameters\n",
    "        s : (list) list where S[i] gives the probability of player i playing each action\n",
    "        payoffs: (np.array) payoffs for i\n",
    "    Returns\n",
    "        eu : (float) expected utility for i\n",
    "    '''\n",
    "    p = p_outcomes(s)\n",
    "    return np.sum(np.multiply(p, payoffs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "payoffs = np.zeros([2, 2, 2])\n",
    "payoffs[0, :, :] = np.array([[0, -1],\n",
    "                            [1, -2]])\n",
    "\n",
    "payoffs[1, :, :] = np.array([[0, 1],\n",
    "                            [-1, -2]])\n",
    "\n",
    "\n",
    "a1 = np.array([0.6, 0.4]) \n",
    "a2 = np.array([.1, 0.9]) \n",
    "assert expected_utility([a1, a2], payoffs=payoffs[0]) == expected_utility([a2, a1], payoffs=payoffs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(3)[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "       [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2, 8)) / (np.ones(8)*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-119.2213150563106"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ll_uniform( dataset):\n",
    "    ll = 0\n",
    "    for play in dataset: \n",
    "        for player in range(2):\n",
    "            traj = play[player]\n",
    "            s = np.ones_like(traj) / np.ones(traj.shape[1])*2\n",
    "            ll += np.log(p_traj(traj, s))\n",
    "    return -ll\n",
    "\n",
    "ll_uniform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Level_K_Model:\n",
    "    def __init__(self, K, h):\n",
    "        self.K = K\n",
    "        self.payoffs = np.zeros([2, 2, 2])\n",
    "        self.payoffs[0, :, :] = np.array([[0, -1],\n",
    "                                          [1, -1.2]])\n",
    "        self.payoffs[1, :, :] = np.array([[0, 1],\n",
    "                                         [-1, -1.2]])\n",
    "        self.alphas = None\n",
    "        self.h = h # the history length\n",
    "\n",
    "\n",
    "    def log_likelihood_independent(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood. This assumes a players level may change as the game goes on\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        \n",
    "        alphas = params[0:-2]\n",
    "        lambda_ = params[-2] \n",
    "        kappa  = params[-1] \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                traj = play[player]\n",
    "                other_traj  = play[1-player] # trajectory of other player\n",
    "                \n",
    "                pred_s = self.predict_traj(traj, other_traj, self.K, lambda_, kappa, overall=True, alphas=alphas)  \n",
    "               \n",
    "                L = traj.shape[1] #length of trajectory\n",
    "                for l in range(L):\n",
    "                    idx = np.where(traj[:, l])\n",
    "                    ll += np.log(pred_s[:, l][idx][0])\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def log_likelihood(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood.\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        alphas = params[0:-2]\n",
    "        lambda_ = params[-2] \n",
    "        kappa  = params[-1] \n",
    "       \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                sum = 0\n",
    "                for i, k in enumerate(range(self.K+1)): # condition on a specific value of k for a player\n",
    "                    alpha_k = alphas[i]\n",
    "    \n",
    "                    traj = play[player]\n",
    "                    other_traj  = play[1-player] # trajectory of other player\n",
    "                    pred_s = self.predict_traj(traj, other_traj, k, lambda_, kappa)\n",
    "                    \n",
    "                    prob = p_traj(traj, pred_s) # probability of that trajectory\n",
    "                    \n",
    "                    sum += alpha_k * prob # this could be 0, so small epsilon added\n",
    "\n",
    "                ll += np.log(sum)\n",
    "                #print(np.log(sum))\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        params = np.zeros(self.K+3) # +2 since level 0 and the lambda parameter, kappa parameter\n",
    "        params[0] = 1 # inital guess is that all players are level 0\n",
    "        params[-2] = 5 # intial guess for lambda\n",
    "        params[-1] = 0.5 # intial guess for kappa\n",
    "\n",
    "        const_arr = np.ones(self.K+3)\n",
    "        const_arr[-1] = 0\n",
    "        const_arr[-2] = 0\n",
    "        #print(const_arr)\n",
    "        constraint = LinearConstraint(const_arr, lb=1, ub=1)\n",
    "        bnds = [(0, 1) for x in range(self.K+1)]\n",
    "        bnds.append((0, 1000)) \n",
    "        bnds.append((0, 1)) \n",
    "        #print(bnds)\n",
    "\n",
    "        result = minimize(\n",
    "            self.log_likelihood_independent ,\n",
    "            params, \n",
    "            args=(dataset),\n",
    "            bounds=bnds, \n",
    "            constraints=constraint) \n",
    "        print(result)\n",
    "\n",
    "        assert result.status == 0 # make sure the optimization was successful\n",
    "        self.alphas = result.x[0:-1]\n",
    "        self.lambda_ = result.x[-1]\n",
    "        ll = result.fun\n",
    "        return ll\n",
    "\n",
    "\n",
    "    def predict_traj(self, traj, other_traj, K, lambda_, kappa, overall=False, alphas=None):\n",
    "        '''\n",
    "        Returns straetgy predictions against other player\n",
    "        \n",
    "        '''\n",
    "        def get_weights(h, kappa):\n",
    "            if h == 1:\n",
    "                return np.ones(1)\n",
    "            \n",
    "            l = [1-kappa]\n",
    "            for i in range(1, h):\n",
    "                l.append(l[i-1]*kappa)\n",
    "\n",
    "            weight_left = 1- np.sum(l)\n",
    "            l[0] = l[0] + weight_left\n",
    "            \n",
    "            l = np.flip(np.array(l))\n",
    "\n",
    "            return l \n",
    "\n",
    "        L = other_traj.shape[1]\n",
    "        pred_i = np.zeros((2, self.K+1, L)) # level-k prediction for each stage game for i\n",
    "        pred_other = np.zeros((2, self.K+1, L)) # level-k prediction for each stage game for other player\n",
    "\n",
    "        for l in range(L):\n",
    "            # first determine level 0 strategy \n",
    "            start_hist_idx = 0 #max(l-self.h, 0)\n",
    "            end_hist_idx = l\n",
    "\n",
    "            if l == 0: # there is no history, so level-0 strategies are uniform\n",
    "                lvl_0_s_i = np.ones((2)) / 2\n",
    "                lvl_0_s_other = np.ones((2)) / 2\n",
    "            else:\n",
    "                hist_i = traj[:, start_hist_idx:end_hist_idx] # limited history of i's actions\n",
    "                hist_other = other_traj[:, start_hist_idx:end_hist_idx] # limited history of -i's actions\n",
    "\n",
    "                w = get_weights(hist_i.shape[1], kappa)\n",
    "                lvl_0_s_i =  0.99*np.dot(hist_i, w)+ 0.01 * np.ones((2))/2\n",
    "                lvl_0_s_other=  0.99*np.dot(hist_other, w)+ 0.01 * np.ones((2))/2\n",
    "\n",
    "\n",
    "            # These become the level 0 strategies\n",
    "            pred_i[:, 0, l] = lvl_0_s_i\n",
    "            pred_other[:, 0, l] = lvl_0_s_other\n",
    "\n",
    "            # Now, for higher levels:\n",
    "            for k in range(1, K+1):\n",
    "                pred_i[:, k, l] = 0.999*self.compute_BR(pred_other[:, k-1, l], lambda_) + 0.001 * np.ones((2))/2\n",
    "                pred_other[:, k, l] =  0.999*self.compute_BR(pred_i[:, k-1, l], lambda_)+ 0.001 * np.ones((2))/2\n",
    " \n",
    "\n",
    "        if overall: # if return the overall prediction\n",
    "            assert alphas is not None\n",
    "            pred_i_ = np.zeros((2, L))\n",
    "\n",
    "            for l in range(L):\n",
    "                pred_i[:, 0, l] = np.ones((2)) / 2\n",
    "                pred_i_[:, l] = np.dot(pred_i[:, :, l], alphas)\n",
    "            pred_i = pred_i_\n",
    "     \n",
    "        else:\n",
    "            pred_i = np.squeeze(pred_i[:, K, :]) # only return the level K predictions\n",
    "\n",
    "\n",
    "        # if K == 0:\n",
    "        #     return np.ones((2, L)) / (np.ones(L)*2)\n",
    "        # else:\n",
    "        return pred_i   \n",
    "\n",
    "\n",
    "    def compute_BR(self,  s_other, lambda_):\n",
    "        '''\n",
    "        Computes a best response\n",
    "\n",
    "        Parameters:\n",
    "            s_other : (np.Array) strategy of other player\n",
    "\n",
    "        NOTE: this ONLY works with symetric payoffs and 2 actions\n",
    "        '''\n",
    "    \n",
    "        #get EU of action 0\n",
    "    \n",
    "        s = [np.array([1, 0]), s_other]\n",
    "        eu_0 = expected_utility(s, self.payoffs[0])\n",
    "        #print('EU 0: {}'.format( eu_0))\n",
    "\n",
    "        # get EU of action 1\n",
    "        s = [np.array([0, 1]), s_other]\n",
    "        eu_1 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        #print('EU 1: {}'.format( eu_1))\n",
    "\n",
    "        # return action with greater EU\n",
    "        return softmax(np.array([eu_0, eu_1])*lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 119.05123206137665\n",
      "     jac: array([-1.71999995e+02, -1.44933311e+02, -1.72000085e+02,  2.95639038e-05,\n",
      "        1.62124634e-05])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 55\n",
      "     nit: 9\n",
      "    njev: 9\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.91015357, 0.        , 0.08984643, 5.42117185, 0.48592246])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = Level_K_Model(2, 3)\n",
    "ll = m.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4447357 , 0.38512707, 0.47786173, 0.41353541, 0.4694578 ],\n",
       "       [0.5552643 , 0.61487293, 0.52213827, 0.58646459, 0.5305422 ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = data[0][0]\n",
    "t2 = data[0][1]\n",
    "# traj, other_traj, K, lambda_, kappa, overall=False, alphas=None):\n",
    "m = Level_K_Model(1, 3)\n",
    "m.predict_traj(t1, t2, 1, 1, 0.5, overall=True, alphas=np.array([0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CH_Model(Level_K_Model):\n",
    "    def log_likelihood_independent(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood. This assumes a players level may change as the game goes on\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        \n",
    "        alphas = params[0:-2]\n",
    "        lambda_ = params[-2] \n",
    "        kappa  = params[-1] \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                traj = play[player]\n",
    "                other_traj  = play[1-player] # trajectory of other player\n",
    "                \n",
    "                pred_s = self.predict_traj(traj, other_traj, self.K, lambda_, kappa, alphas=alphas)  \n",
    "               \n",
    "                L = traj.shape[1] #length of trajectory\n",
    "                for l in range(L):\n",
    "                    idx = np.where(traj[:, l])\n",
    "                    ll += np.log(pred_s[:, l][idx][0])\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "    def log_likelihood(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood.\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        alphas = params[0:-1]\n",
    "        lambda_ = params[-1] \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                sum = 0\n",
    "                for i, k in enumerate(range(self.K+1)): # condition on a specific value of k for a player\n",
    "                    alpha_k = alphas[i]\n",
    "                    traj = play[player]\n",
    "                    other_traj  = play[1-player] # trajectory of other player   \n",
    "                          \n",
    "                    pred_s = self.predict_traj(traj, other_traj, k, lambda_, alphas)\n",
    "                    prob = p_traj(traj, pred_s) # probability of that trajectory\n",
    "                    sum += alpha_k * (prob + 0.001) # this could be 0, so small epsilon added\n",
    "\n",
    "                assert sum != 0\n",
    "                ll += np.log(sum)\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "    def fit(self, dataset):\n",
    "\n",
    "\n",
    "        params = np.zeros(self.K+3) # +2 since level 0 and the lambda parameter\n",
    "        params[0] = 1 # inital guess is that all players are level 0\n",
    "        params[-2] = 5 # intial guess for lambda\n",
    "        params[-1] = 5 # intial guess for kappa\n",
    "\n",
    "        const_arr = np.ones(self.K+3)\n",
    "        const_arr[-2] = 0\n",
    "        const_arr[-1] = 0\n",
    "        #print(const_arr)\n",
    "        constraint = LinearConstraint(const_arr, lb=1, ub=1)\n",
    "        bnds = [(0, 1) for x in range(self.K+1)]\n",
    "        bnds.append((0, 1000)) # no bounds for lambda\n",
    "        bnds.append((0, 1)) # no bounds for kappa\n",
    "  \n",
    "        result = minimize(\n",
    "            self.log_likelihood_independent ,\n",
    "            params, \n",
    "            args=(dataset),\n",
    "            bounds=bnds, \n",
    "            constraints=constraint) \n",
    "        print(result)\n",
    "\n",
    "        assert result.status == 0 # make sure the optimization was successful\n",
    "\n",
    "\n",
    "    def predict_traj(self, traj, other_traj, K, lambda_, kappa, alphas):\n",
    "        '''\n",
    "        Returns straetgy predictions against other player\n",
    "        \n",
    "        '''\n",
    "\n",
    "        def get_weights(h, kappa):\n",
    "            if h == 1:\n",
    "                return np.ones(1)\n",
    "            \n",
    "            l = [1-kappa]\n",
    "            for i in range(1, h-1):\n",
    "                l.append(l[i-1]*kappa)\n",
    "\n",
    "            l.append(1- np.sum(l))\n",
    "            #assert np.sum(l) == 1\n",
    "            l = np.flip(np.array(l))\n",
    "\n",
    "            return l \n",
    "\n",
    "\n",
    "        L = other_traj.shape[1]\n",
    "        pred_i = np.zeros((2, self.K+1, L)) # level-k prediction for each stage game for i\n",
    "        pred_other = np.zeros((2, self.K+1, L)) # level-k prediction for each stage game for other player\n",
    "\n",
    "        for l in range(L):\n",
    "            # first determine level 0 strategy \n",
    "            start_hist_idx = max(l-self.h, 0)\n",
    "            end_hist_idx = l\n",
    "\n",
    "            if l == 0: # there is no history, so level-0 strategies are uniform\n",
    "                lvl_0_s_i = np.ones((2)) / 2\n",
    "                lvl_0_s_other = np.ones((2)) / 2\n",
    "            else:\n",
    "                hist_i = traj[:, start_hist_idx:end_hist_idx] # limited history of i's actions\n",
    "                hist_other = other_traj[:, start_hist_idx:end_hist_idx] # limited history of -i's actions\n",
    "                \n",
    "                w = get_weights(hist_i.shape[1], kappa)\n",
    "                lvl_0_s_i =  0.99*np.dot(hist_i, w)+ 0.01 * np.ones((2))/2\n",
    "                lvl_0_s_other=  0.99*np.dot(hist_other, w)+ 0.01 * np.ones((2))/2\n",
    "            \n",
    "            # These become the level 0 strategies\n",
    "            pred_i[:, 0, l] = lvl_0_s_i\n",
    "            pred_other[:, 0, l] = lvl_0_s_other\n",
    "\n",
    "            # Now, for higher levels:\n",
    "            for k in range(1, K+1):\n",
    "                if np.sum(alphas[0:k]) != 0:\n",
    "                    s_other = np.matmul(pred_other[:, 0:k, l], alphas[0:k]) / np.sum(alphas[0:k])\n",
    "                    s_i = np.matmul(pred_i[:, 0:k, l], alphas[0:k]) / np.sum(alphas[0:k])\n",
    "                else:\n",
    "                    s_other = np.matmul(pred_other[:, 0:k, l], np.ones(k)/k) \n",
    "                    s_i = np.matmul(pred_i[:, 0:k, l], np.ones(k)/k) \n",
    "                    \n",
    "                pred_i[:, k, l] = self.compute_BR(s_other, lambda_)  \n",
    "                pred_other[:, k, l] = self.compute_BR(s_i, lambda_)\n",
    " \n",
    "\n",
    "        pred_i = np.squeeze(pred_i[:, K, :]) # only return the level K predictions\n",
    "        return pred_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0.4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array([[0.3, 0.7], [0.5, 0.5], [0.9, 0.1]]).T\n",
    "w = [0.2, 0.3, 0.3]\n",
    "np.matmul(pi, w) / np.sum(w )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 119.22131505631083\n",
      "     jac: array([0., 0., 0., 0., 0., 0.])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 14\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.93190602e-08, 4.99999981e-01, 4.99999981e-01, 1.93541515e-08,\n",
      "       6.59089494e-09, 4.08512051e-08])\n"
     ]
    }
   ],
   "source": [
    "m = CH_Model(3, 4)\n",
    "\n",
    "t1 = data[0][0]\n",
    "t2 = data[0][1]\n",
    "\n",
    "r = m.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reason about future rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Model:\n",
    "    def __init__(self, h=1):\n",
    "        self.payoffs = np.zeros([2, 2, 2])\n",
    "        self.payoffs[0, :, :] = np.array([[0, -1],\n",
    "                                          [1, -1]])\n",
    "        self.payoffs[1, :, :] = np.array([[0, 1],\n",
    "                                         [-1, -1]])\n",
    "        self.alphas = None\n",
    "        self.h = h # the history length\n",
    "\n",
    "\n",
    "    def log_likelihood(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood.\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        alphas = params[0:3] # frequency of different risk types in population\n",
    "        gammas = params[3:6] # parameter of risk types in population\n",
    "        lambda_ = params[-1] \n",
    "      \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                sum = 0\n",
    "                for i, alpha_gamma in enumerate(alphas): # condition on a specific type of  player\n",
    "                    traj = play[player]\n",
    "                    other_traj  = play[1-player] # trajectory of other player\n",
    "                    gamma = gammas[i]\n",
    "                    pred_s = self.predict_traj(traj, other_traj, lambda_, gamma)\n",
    "                    prob = p_traj(traj, pred_s) # probability of that trajectory\n",
    "                    sum += alpha_gamma * (prob) # this could be 0, so small epsilon added\n",
    "                ll += np.log(sum)\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        params = np.zeros(7) # +2 since level 0 and the lambda parameter\n",
    "    \n",
    "        for i in range(3):\n",
    "            params[i] = 1/3\n",
    "\n",
    "        params[3] = 0.5\n",
    "        params[4] = 1\n",
    "        params[5] = 2\n",
    "        params[6] = 1 # intial guess for lambda\n",
    "\n",
    "        const_arr = np.zeros(7)\n",
    "        for i in range(3):\n",
    "            const_arr[i] = 1\n",
    "\n",
    "       \n",
    "        constraint = LinearConstraint(const_arr, lb=1, ub=1)\n",
    "        bnds = [(0, 1) for x in range(3)]\n",
    "       \n",
    "        bnds.append((-1000, 1))\n",
    "        bnds.append((1, 1))\n",
    "        bnds.append((1, 1000))\n",
    "        bnds.append((0, 1000)) # no bounds for lambda\n",
    "       \n",
    "        result = minimize(\n",
    "            self.log_likelihood ,\n",
    "            params, \n",
    "            args=(dataset),\n",
    "            bounds=bnds, \n",
    "            constraints=constraint,\n",
    "            method='SLSQP' )\n",
    "        print(result)\n",
    "\n",
    "        assert result.status == 0 # make sure the optimization was successful\n",
    "    \n",
    "    \n",
    "        \n",
    "    def predict_traj(self, traj, other_traj, lambda_, gamma): \n",
    "        '''\n",
    "        I think about what action I can take\n",
    "\n",
    "        what do I think you will play this round? \n",
    "        -> probably a best response to my last action\n",
    "\n",
    "        what do I think you will play next round?\n",
    "        -> probably a brest resonse to this aciton\n",
    "\n",
    "        what will I play next round? \n",
    "            -> probably a best resonse to what I think you will play next round\n",
    "        '''\n",
    "        L = other_traj.shape[1]\n",
    "        pred_i = np.zeros((2, L)) # level-k prediction for each stage game for i\n",
    "\n",
    "        for l in range(L):\n",
    "\n",
    "            # I could extend this to include history\n",
    "            if l == 0:\n",
    "                # if action 0\n",
    "                s_other_next = self.BR(np.array([1,0]), lambda_)\n",
    "                s_i_next = self.BR(s_other_next, lambda_)\n",
    "                eu_0 = expected_utility([np.array([1,0]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                # if action 1\n",
    "                s_other_next = self.BR(np.array([0,1]), lambda_)\n",
    "                s_i_next = self.BR(s_other_next, lambda_)\n",
    "                eu_1 =  expected_utility([np.array([0,1]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "           \n",
    "            else:\n",
    "                # if action 0\n",
    "                s_other_curr = self.BR(traj[:, l-1], lambda_)\n",
    "                s_other_next = self.BR(np.array([1,0]), lambda_)\n",
    "                s_i_next = self.BR(s_other_next, lambda_)\n",
    "                eu_0 = expected_utility([np.array([1,0]), s_other_curr], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                # if action 1\n",
    "                s_other_curr = self.BR(traj[:, l-1], lambda_)\n",
    "                s_other_next = self.BR(np.array([0,1]), lambda_)\n",
    "                s_i_next = self.BR(s_other_next, lambda_)\n",
    "                eu_1 = expected_utility([np.array([0,1]), s_other_curr], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "            pred_i[:, l]  = softmax(lambda_* np.array([eu_0, eu_1]))\n",
    "\n",
    "        return pred_i   \n",
    "\n",
    "\n",
    "    def BR(self,  s_other, lambda_):\n",
    "        '''\n",
    "        Computes a best response\n",
    "\n",
    "        Parameters:\n",
    "            s_other : (np.Array) strategy profile of other player\n",
    "\n",
    "        NOTE: this ONLY works with symetric payoffs and 2 actions\n",
    "        '''\n",
    "    \n",
    "        #get EU of action 0\n",
    "    \n",
    "        s = [np.array([1, 0]), s_other]\n",
    "        eu_0 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        # get EU of action 1\n",
    "        s = [np.array([0, 1]), s_other]\n",
    "        eu_1 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "\n",
    "        # if eu_0 >= eu_1:\n",
    "        #     return np.array([1,0])\n",
    "        # else:\n",
    "        #     return np.array([0,1])\n",
    "\n",
    "        # return action with greater EU\n",
    "        return softmax(np.array([eu_0, eu_1])*lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 119.22131505631016\n",
      "     jac: array([-28.        , -28.        , -28.        ,   0.        ,\n",
      "                nan,   0.        ,   2.49999905])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 14\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 1.00000000e+00,  0.00000000e+00,  7.46069873e-13, -6.90858269e+00,\n",
      "        1.00000000e+00,  1.00000000e+00,  2.54374299e-12])\n"
     ]
    }
   ],
   "source": [
    "m = Temporal_Model()\n",
    "\n",
    "t1 = data[0][0]\n",
    "t2 = data[0][1]\n",
    "\n",
    "\n",
    "# alphas = params[0:3] # frequency of different risk types in population\n",
    "# gammas = params[3:6] # parameter of risk types in population\n",
    "# lambda_ = params[-1] \n",
    "r = m.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_History_Model:\n",
    "    def __init__(self, h=1):\n",
    "        self.payoffs = np.zeros([2, 2, 2])\n",
    "        self.payoffs[0, :, :] = np.array([[0, -1],\n",
    "                                          [1, -2]])\n",
    "        self.payoffs[1, :, :] = np.array([[0, 1],\n",
    "                                         [-1, -2]])\n",
    "        self.alphas = None\n",
    "        self.h = h # the history length\n",
    "\n",
    "\n",
    "\n",
    "    def log_likelihood(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood.\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        alphas = params[0:3] # frequency of different risk types in population\n",
    "        gammas = params[3:6] # parameter of risk types in population\n",
    "        lambda_ = params[-2] \n",
    "        kappa = params[-1] \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                sum = 0\n",
    "                for i, alpha_gamma in enumerate(alphas): # condition on a specific type of  player\n",
    "                    traj = play[player]\n",
    "                    other_traj  = play[1-player] # trajectory of other player\n",
    "                    gamma = gammas[i]\n",
    "                    pred_s = self.predict_traj(traj, other_traj, lambda_, gamma, kappa)\n",
    "                    prob = p_traj(traj, pred_s) # probability of that trajectory\n",
    "                    sum += alpha_gamma * (prob) # this could be 0, so small epsilon added\n",
    "                ll += np.log(sum)\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        params = np.zeros(8)\n",
    "    \n",
    "        for i in range(3):\n",
    "            params[i] = 1/3\n",
    "\n",
    "        params[3] = 0.5\n",
    "        params[4] = 1\n",
    "        params[5] = 2\n",
    "        params[6] = 1 # intial guess for lambda\n",
    "        params[7] = 0.8 # intial guess for kappa\n",
    "\n",
    "        const_arr = np.zeros(8)\n",
    "        for i in range(3):\n",
    "            const_arr[i] = 1\n",
    "\n",
    "       \n",
    "        constraint = LinearConstraint(const_arr, lb=1, ub=1)\n",
    "        bnds = [(0, 1) for x in range(3)]\n",
    "       \n",
    "        bnds.append((-1000, 1))\n",
    "        bnds.append((1, 1))\n",
    "        bnds.append((1, 1000))\n",
    "        bnds.append((0, 1000)) # bounds for lambda\n",
    "        bnds.append((0, 1)) # bounds for kappa\n",
    "       \n",
    "        result = minimize(\n",
    "            self.log_likelihood ,\n",
    "            params, \n",
    "            args=(dataset),\n",
    "            bounds=bnds, \n",
    "            constraints=constraint) \n",
    "        print(result)\n",
    "\n",
    "        assert result.status == 0 # make sure the optimization was successful\n",
    "    \n",
    "    \n",
    "        \n",
    "    def predict_traj(self, traj, other_traj, lambda_, gamma, kappa): \n",
    "        '''\n",
    "        I think about what action I can take\n",
    "\n",
    "        what do I think you will play this round? \n",
    "        -> probably a best response to my last action\n",
    "\n",
    "        what do I think you will play next round?\n",
    "        -> probably a brest resonse to this aciton\n",
    "\n",
    "        what will I play next round? \n",
    "            -> probably a best resonse to what I think you will play next round\n",
    "        '''\n",
    "\n",
    "        def get_weights(h, kappa):\n",
    "            if h == 1:\n",
    "                return np.ones(1)\n",
    "            \n",
    "            l = [1-kappa]\n",
    "            for i in range(1, h-1):\n",
    "                l.append(l[i-1]*kappa)\n",
    "\n",
    "            l.append(1- np.sum(l))\n",
    "            #assert np.sum(l) == 1\n",
    "            l = np.flip(np.array(l))\n",
    "\n",
    "            return l \n",
    "\n",
    "        L = other_traj.shape[1]\n",
    "        pred_i = np.zeros((2, L)) # level-k prediction for each stage game for i\n",
    "\n",
    "\n",
    "        for l in range(L):\n",
    "\n",
    "            # I could extend this to include history\n",
    "            if l == 0:\n",
    "                # if action 0\n",
    "                s_other_next = self.BR(np.array([1,0]), lambda_)\n",
    "                s_i_next = self.BR(s_other_next, lambda_)\n",
    "                eu_0 = expected_utility([np.array([1,0]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                # if action 1\n",
    "                s_other_next = self.BR(np.array([0,1]), lambda_)\n",
    "                s_i_next = self.BR(s_other_next, lambda_)\n",
    "                eu_1 =  expected_utility([np.array([0,1]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "           \n",
    "            else:\n",
    "               \n",
    "                def predicted_eu(action, hist_i):\n",
    "                    nonlocal lambda_\n",
    "                    nonlocal kappa\n",
    "                    w = get_weights(hist_i.shape[1], kappa)\n",
    "                    s_other_curr = self.BR(np.dot(hist_i, w), lambda_)\n",
    "\n",
    "                    hist_i_next = np.c_[hist_i, action]\n",
    "                    w = get_weights(hist_i_next.shape[1], kappa)\n",
    "                    s_other_next = self.BR(np.dot(hist_i_next, w), lambda_)\n",
    "\n",
    "                    s_i_next = self.BR(s_other_next, lambda_)\n",
    "                    eu = expected_utility([action, s_other_curr], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "                    return eu\n",
    "\n",
    "                #print(eu_0)\n",
    "\n",
    "                eu_0 = predicted_eu(np.array([1, 0]), traj[:, 0:l])\n",
    "                eu_1 = predicted_eu(np.array([0, 1]), traj[:, 0:l])\n",
    "               \n",
    "              #  print(eu_1)\n",
    "            pred_i[:, l]  = softmax(lambda_* np.array([eu_0, eu_1]))\n",
    "\n",
    "        return pred_i   \n",
    "\n",
    "\n",
    "    def BR(self,  s_other, lambda_):\n",
    "        '''\n",
    "        Computes a best response\n",
    "\n",
    "        Parameters:\n",
    "            s_other : (np.Array) strategy profile of other player\n",
    "\n",
    "        NOTE: this ONLY works with symetric payoffs and 2 actions\n",
    "        '''\n",
    "    \n",
    "        #get EU of action 0\n",
    "    \n",
    "        s = [np.array([1, 0]), s_other]\n",
    "        eu_0 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        # get EU of action 1\n",
    "        s = [np.array([0, 1]), s_other]\n",
    "        eu_1 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        return softmax(np.array([eu_0, eu_1])*lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 114.69670137451106\n",
      "     jac: array([-2.80000658e+01, -2.75068636e+01, -2.79998741e+01, -1.23977661e-05,\n",
      "                   nan,  7.15255737e-05,  8.31604004e-04,  5.81382751e-01])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 256\n",
      "     nit: 31\n",
      "    njev: 31\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.64230928,  0.        ,  0.35769072, -2.08531107,  1.        ,\n",
      "        2.32722277,  0.57139714,  0.        ])\n"
     ]
    }
   ],
   "source": [
    "m = Temporal_History_Model()\n",
    "r = m.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reason about future and I have a level of reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Temporal_Level_Model:\n",
    "    def __init__(self):\n",
    "        self.payoffs = np.zeros([2, 2, 2])\n",
    "        self.payoffs[0, :, :] = np.array([[0, -1],\n",
    "                                          [1, -1]])\n",
    "        self.payoffs[1, :, :] = np.array([[0, 1],\n",
    "                                         [-1, -1]])\n",
    "        self.alphas = None\n",
    "       \n",
    "\n",
    "    def log_likelihood(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood.\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        alphas = params[0:3] # frequency of different reasoning types in the population\n",
    "        alpha_gammas = params[3:6]  # parameter for forward looking type\n",
    "        gammas = params[6:9]\n",
    "        \n",
    "        lambda_ = params[-2] \n",
    "        kappa = params[-1] \n",
    "      \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                sum = 0\n",
    "                for k, alpha_k in enumerate(alphas): # condition on a specific type of  player\n",
    "                    for i, alpha_gamma in enumerate(alpha_gammas): # condition on a specific type of  player\n",
    "                        traj = play[player]\n",
    "                        other_traj  = play[1-player] # trajectory of other player\n",
    "                        gamma = gammas[i]\n",
    "                        pred_s = self.predict_traj(traj, other_traj, lambda_, gamma, alphas, kappa, k)\n",
    "                        prob = p_traj(traj, pred_s) # probability of that trajectory\n",
    "                        sum += alpha_k * alpha_gamma* (prob) # this could be 0, so small epsilon added\n",
    "                ll += np.log(sum)\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "        # ll = 0\n",
    "        # for play in dataset: \n",
    "        #     for player in range(2):\n",
    "        #         traj = play[player]\n",
    "        #         other_traj  = play[1-player] # trajectory of other player    \n",
    "        #         pred_s = self.predict_traj(traj, other_traj, lambda_, gamma, alphas, kappa)\n",
    "                   \n",
    "        #         L = traj.shape[1] #length of trajectory\n",
    "        #         for l in range(L):\n",
    "        #             idx = np.where(traj[:, l])\n",
    "        #             ll += np.log(pred_s[:, l][idx][0])\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def predict(self, params):\n",
    "        alphas = params[0:3] # frequency of different reasoning types in the population\n",
    "        alpha_gammas = params[3:6]  # parameter for forward looking type\n",
    "        gammas = params[6:9]\n",
    "        \n",
    "        lambda_ = params[-2] \n",
    "        kappa = params[-1] \n",
    "\n",
    "\n",
    "        pred_s = self.predict_traj(traj, other_traj, lambda_, gamma, alphas, kappa, k)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        params = np.zeros(11)\n",
    "    \n",
    "        for i in range(6):\n",
    "            params[i] = 1/3\n",
    "\n",
    "        params[7] = 0 # lambda\n",
    "        params[8] = 1 # lambda\n",
    "        params[9] = 10 # lambda\n",
    "        params[10] = 1 # lambda\n",
    "    \n",
    "\n",
    "        const_arr1 = np.zeros(11)\n",
    "        for i in range(3):\n",
    "            const_arr1[i] = 1\n",
    "       \n",
    "        constraint1 = LinearConstraint(const_arr1, lb=1, ub=1)\n",
    "       \n",
    "        const_arr2= np.zeros(11)\n",
    "        for i in range(3, 6):\n",
    "            const_arr2[i] = 1\n",
    "       \n",
    "        constraint2 = LinearConstraint(const_arr2, lb=1, ub=1)\n",
    "        \n",
    "        bnds = [(0, 1) for x in range(6)]\n",
    "        bnds.append((0, 1)) # gamma 1\n",
    "        bnds.append((1, 1)) # kgamma 2\n",
    "        bnds.append((1, 100)) # gamma 2\n",
    "        bnds.append((0, 1000)) # lambda\n",
    "        bnds.append((0, 1)) # kappa\n",
    "       \n",
    "        result = minimize(\n",
    "            self.log_likelihood ,\n",
    "            params, \n",
    "            args=(dataset),\n",
    "            bounds=bnds, \n",
    "            constraints=[constraint1, constraint2]) \n",
    "        print(result)\n",
    "\n",
    "        assert result.status == 0 # make sure the optimization was successful\n",
    "        return result\n",
    "    \n",
    "        \n",
    "    def predict_traj(self, traj, other_traj, lambda_, gamma, alphas, kappa, k): \n",
    "        '''\n",
    "        I think about what action I can take\n",
    "\n",
    "        what do I think you will play this round? \n",
    "        -> probably a best response to my last action\n",
    "\n",
    "        what do I think you will play next round?\n",
    "        -> probably a brest resonse to this aciton\n",
    "\n",
    "        what will I play next round? \n",
    "            -> probably a best resonse to what I think you will play next round\n",
    "        '''\n",
    "\n",
    "\n",
    "        L = other_traj.shape[1]\n",
    "        pred_i = np.zeros((2, len(alphas), L)) # level-k prediction for each stage game for \n",
    "        \n",
    "        def get_weights(h, kappa):\n",
    "            if h == 1:\n",
    "                return np.ones(1)\n",
    "            \n",
    "            l = [1-kappa]\n",
    "            for i in range(1, h):\n",
    "                l.append(l[i-1]*kappa)\n",
    "\n",
    "            weight_left = 1- np.sum(l)\n",
    "            l[0] = l[0] + weight_left\n",
    "            \n",
    "            l = np.flip(np.array(l))\n",
    "\n",
    "            return l \n",
    "\n",
    "        for l in range(L):\n",
    "            for k in range(len(alphas)):\n",
    "                if k == 0:\n",
    "                    pred_i[:, k, l] = np.ones(2)/2\n",
    "\n",
    "                if k == 1:\n",
    "                    \n",
    "                    if l == 0:\n",
    "                        # if action 0                   \n",
    "                        eu_0 = expected_utility([np.array([1,0]), np.array([0.5, 0.5])], self.payoffs[0]) \n",
    "                        # if action 1                 \n",
    "                        eu_1 =  expected_utility([np.array([0,1]), np.array([0.5, 0.5])], self.payoffs[0])\n",
    "                \n",
    "                    else:\n",
    "                        hist = traj[:, 0:l]\n",
    "                        w = get_weights(hist.shape[1], kappa)\n",
    "                        s_other_curr = self.BR(np.dot(hist, w), lambda_)\n",
    "                        # if action 0\n",
    "                        eu_0 = expected_utility([np.array([1,0]), s_other_curr], self.payoffs[0])\n",
    "                        # if action 1\n",
    "                        eu_1 = expected_utility([np.array([0,1]), s_other_curr], self.payoffs[0]) \n",
    "\n",
    "                    \n",
    "                    pred_i[:, k, l]  = softmax(lambda_* np.array([eu_0, eu_1]))\n",
    "\n",
    "                if k == 2:\n",
    "                    if l == 0:\n",
    "                        # if action 0\n",
    "                        s_other_next = self.BR(np.array([1,0]), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "                        eu_0 = expected_utility([np.array([1,0]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                        # if action 1\n",
    "                        s_other_next = self.BR(np.array([0,1]), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "                        eu_1 =  expected_utility([np.array([0,1]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "                \n",
    "                    else:\n",
    "                        # if action 0\n",
    "\n",
    "                        hist = traj[:, 0:l]\n",
    "                        w = get_weights(hist.shape[1], kappa)\n",
    "        \n",
    "                        s_other_curr = self.BR(np.dot(hist, w), lambda_)\n",
    "\n",
    "                        hist_new = np.c_[hist, np.array([1,0]) ]\n",
    "                        w = get_weights(hist_new.shape[1], kappa)\n",
    "                        s_other_next = self.BR(np.dot(hist_new, w), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "\n",
    "                        eu_0 = expected_utility([np.array([1,0]), s_other_curr], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                        # if action 1\n",
    "\n",
    "                        hist_new = np.c_[hist, np.array([0,1]) ]\n",
    "                        w = get_weights(hist_new.shape[1], kappa)\n",
    "                        s_other_next = self.BR(np.dot(hist_new, w), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "\n",
    "                        eu_1 = expected_utility([np.array([0,1]), s_other_curr], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                    pred_i[:,k, l]  = softmax(lambda_* np.array([eu_0, eu_1]))\n",
    "\n",
    "\n",
    "        pred_i_ = np.zeros((2, L))\n",
    "\n",
    "        # for l in range(L):\n",
    "        #     pred_i_[:, l] = np.dot(pred_i[:, :, l], alphas)\n",
    "        # pred_i = pred_i_\n",
    "\n",
    "        pred_i = np.squeeze(pred_i[:, k, :])\n",
    "        \n",
    "        return pred_i   \n",
    "\n",
    "\n",
    "    def BR(self,  s_other, lambda_):\n",
    "        '''\n",
    "        Computes a best response\n",
    "\n",
    "        Parameters:\n",
    "            s_other : (np.Array) strategy profile of other player\n",
    "\n",
    "        NOTE: this ONLY works with symetric payoffs and 2 actions\n",
    "        '''\n",
    "    \n",
    "        #get EU of action 0\n",
    "    \n",
    "        s = [np.array([1, 0]), s_other]\n",
    "        eu_0 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        # get EU of action 1\n",
    "        s = [np.array([0, 1]), s_other]\n",
    "        eu_1 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        return softmax(np.array([eu_0, eu_1])*lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 119.22131506645408\n",
      "     jac: array([-28. , -28. , -28. , -28. , -28. , -28. ,   0. ,   nan,   0. ,\n",
      "         2.5,   0. ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 22\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([3.33333333e-01, 3.33333333e-01, 3.33333333e-01, 9.99999983e-01,\n",
      "       8.54528648e-09, 8.54487797e-09, 1.79405561e-07, 1.00000000e+00,\n",
      "       1.00000000e+00, 4.05739975e-09, 4.27333134e-07])\n"
     ]
    }
   ],
   "source": [
    "m = Temporal_Level_Model()\n",
    "m.fit(data)\n",
    "t1 = data[0][0]\n",
    "t2 = data[0][1]\n",
    "\n",
    "\n",
    "# print(t1)\n",
    "#print(m.predict_traj(t1, t2, lambda_=1, gamma=1, k=1))\n",
    "# print(t2)\n",
    "# # m.predict_traj(t1, t2, 2, 5, overall=True, alphas=[0, 1, 0])\n",
    "\n",
    "#m.log_likelihood( [1, 0, 0, 1, 1, 0.25], data)\n",
    "# print(m.predict_traj(t2, t1, 2, 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Temporal_Level_Model:\n",
    "    def __init__(self):\n",
    "        self.payoffs = np.zeros([2, 2, 2])\n",
    "        self.payoffs[0, :, :] = np.array([[0.5, -1],\n",
    "                                          [1, 0]])\n",
    "        self.payoffs[1, :, :] = np.array([[0.5, 1],\n",
    "                                         [-1, 0]])\n",
    "        self.alphas = None\n",
    "       \n",
    "\n",
    "\n",
    "    def log_likelihood(self, params, dataset):\n",
    "        '''\n",
    "        Given the dataset and the fitted model, returns the log likelihood.\n",
    "\n",
    "        NOTE: assumes each player has a fixed level\n",
    "        \n",
    "        Parameters:\n",
    "            params : np.array of alpha_ks, the freq of that level in population and lambda_ for QBR\n",
    "            dataset : list of plays, each play is an np array\n",
    "        Returns:\n",
    "            ll : loglikelihood of dataset\n",
    "\n",
    "        '''\n",
    "        alphas = params[0:3] # frequency of different reasoning types in the population\n",
    "        \n",
    "        gamma = params[-3]  # parameter for forward looking type\n",
    "        lambda_ = params[-2] \n",
    "        kappa = params[-1] \n",
    "      \n",
    "\n",
    "        ll = 0\n",
    "        for play in dataset: \n",
    "            for player in range(2):\n",
    "                sum = 0\n",
    "                for k, alpha_k in enumerate(alphas): # condition on a specific type of  player\n",
    "                    traj = play[player]\n",
    "                    other_traj  = play[1-player] # trajectory of other player\n",
    "                  \n",
    "                    pred_s = self.predict_traj(traj, other_traj, lambda_, gamma, alphas, kappa, k)\n",
    "                    prob = p_traj(traj, pred_s) # probability of that trajectory\n",
    "                    sum += alpha_k * (prob) # this could be 0, so small epsilon added\n",
    "                ll += np.log(sum)\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "        # ll = 0\n",
    "        # for play in dataset: \n",
    "        #     for player in range(2):\n",
    "        #         traj = play[player]\n",
    "        #         other_traj  = play[1-player] # trajectory of other player    \n",
    "        #         pred_s = self.predict_traj(traj, other_traj, lambda_, gamma, alphas, kappa)\n",
    "                   \n",
    "        #         L = traj.shape[1] #length of trajectory\n",
    "        #         for l in range(L):\n",
    "        #             idx = np.where(traj[:, l])\n",
    "        #             ll += np.log(pred_s[:, l][idx][0])\n",
    "\n",
    "        return -ll # since we are minimizing the negative log likelihood\n",
    "\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        params = np.zeros(6) # +2 since level 0 and the lambda parameter\n",
    "    \n",
    "        for i in range(3):\n",
    "            params[i] = 1/3\n",
    "\n",
    "        params[3] = 0.5 #gamma \n",
    "        params[4] = 1 # lambda\n",
    "        params[5] = 0.5 # kappa\n",
    "\n",
    "        const_arr = np.zeros(6)\n",
    "        for i in range(3):\n",
    "            const_arr[i] = 1\n",
    "       \n",
    "        constraint = LinearConstraint(const_arr, lb=1, ub=1)\n",
    "        bnds = [(0, 1) for x in range(3)]\n",
    "       \n",
    "        bnds.append((0, 1)) \n",
    "        bnds.append((0, 1000)) # lambda\n",
    "        bnds.append((0, 1)) \n",
    "       \n",
    "        result = minimize(\n",
    "            self.log_likelihood ,\n",
    "            params, \n",
    "            args=(dataset),\n",
    "            bounds=bnds, \n",
    "            constraints=constraint) \n",
    "        #print(result)\n",
    "\n",
    "        assert result.status == 0 # make sure the optimization was successful\n",
    "        return result\n",
    "    \n",
    "        \n",
    "    def predict_traj(self, traj, other_traj, lambda_, gamma, alphas, kappa, k): \n",
    "        '''\n",
    "        I think about what action I can take\n",
    "\n",
    "        what do I think you will play this round? \n",
    "        -> probably a best response to my last action\n",
    "\n",
    "        what do I think you will play next round?\n",
    "        -> probably a brest resonse to this aciton\n",
    "\n",
    "        what will I play next round? \n",
    "            -> probably a best resonse to what I think you will play next round\n",
    "        '''\n",
    "\n",
    "\n",
    "        L = other_traj.shape[1]\n",
    "        pred_i = np.zeros((2, len(alphas), L)) # level-k prediction for each stage game for \n",
    "        \n",
    "        def get_weights(h, kappa):\n",
    "            if h == 1:\n",
    "                return np.ones(1)\n",
    "            \n",
    "            l = [1-kappa]\n",
    "            for i in range(1, h):\n",
    "                l.append(l[i-1]*kappa)\n",
    "\n",
    "            weight_left = 1- np.sum(l)\n",
    "            l[0] = l[0] + weight_left\n",
    "            \n",
    "            l = np.flip(np.array(l))\n",
    "\n",
    "            return l \n",
    "\n",
    "        for l in range(L):\n",
    "            for k in range(len(alphas)):\n",
    "                if k == 0:\n",
    "                    pred_i[:, k, l] = np.ones(2)/2\n",
    "\n",
    "                if k == 1:\n",
    "                    \n",
    "                    if l == 0:\n",
    "                        # if action 0                   \n",
    "                        eu_0 = expected_utility([np.array([1,0]), np.array([0.5, 0.5])], self.payoffs[0]) \n",
    "                        # if action 1                 \n",
    "                        eu_1 =  expected_utility([np.array([0,1]), np.array([0.5, 0.5])], self.payoffs[0])\n",
    "                \n",
    "                    else:\n",
    "                        hist = traj[:, 0:l]\n",
    "                        w = get_weights(hist.shape[1], kappa)\n",
    "                        s_other_curr = self.BR(np.dot(hist, w), lambda_)\n",
    "                        # if action 0\n",
    "                        eu_0 = expected_utility([np.array([1,0]), s_other_curr], self.payoffs[0])\n",
    "                        # if action 1\n",
    "                        eu_1 = expected_utility([np.array([0,1]), s_other_curr], self.payoffs[0]) \n",
    "\n",
    "                    \n",
    "                    pred_i[:, k, l]  = softmax(lambda_* np.array([eu_0, eu_1]))\n",
    "\n",
    "                if k == 2:\n",
    "                    if l == 0:\n",
    "                        # if action 0\n",
    "                        s_other_next = self.BR(np.array([1,0]), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "                        eu_0 = expected_utility([np.array([1,0]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                        # if action 1\n",
    "                        s_other_next = self.BR(np.array([0,1]), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "                        eu_1 =  expected_utility([np.array([0,1]), np.array([0.5, 0.5])], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "                \n",
    "                    else:\n",
    "                        # if action 0\n",
    "\n",
    "                        hist = traj[:, 0:l]\n",
    "                        w = get_weights(hist.shape[1], kappa)\n",
    "        \n",
    "                        s_other_curr = self.BR(np.dot(hist, w), lambda_)\n",
    "\n",
    "                        hist_new = np.c_[hist, np.array([1,0]) ]\n",
    "                        w = get_weights(hist_new.shape[1], kappa)\n",
    "                        s_other_next = self.BR(np.dot(hist_new, w), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "\n",
    "                        eu_0 = expected_utility([np.array([1,0]), s_other_curr], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                        # if action 1\n",
    "\n",
    "                        hist_new = np.c_[hist, np.array([0,1]) ]\n",
    "                        w = get_weights(hist_new.shape[1], kappa)\n",
    "                        s_other_next = self.BR(np.dot(hist_new, w), lambda_)\n",
    "                        s_i_next = self.BR(s_other_next, lambda_)\n",
    "\n",
    "                        eu_1 = expected_utility([np.array([0,1]), s_other_curr], self.payoffs[0]) + gamma*expected_utility([s_i_next, s_other_next], self.payoffs[0])\n",
    "\n",
    "                    pred_i[:,k, l]  = softmax(lambda_* np.array([eu_0, eu_1]))\n",
    "\n",
    "\n",
    "        pred_i_ = np.zeros((2, L))\n",
    "\n",
    "        # for l in range(L):\n",
    "        #     pred_i_[:, l] = np.dot(pred_i[:, :, l], alphas)\n",
    "        # pred_i = pred_i_\n",
    "\n",
    "        pred_i = np.squeeze(pred_i[:, k, :])\n",
    "        \n",
    "        return pred_i   \n",
    "\n",
    "\n",
    "    def BR(self,  s_other, lambda_):\n",
    "        '''\n",
    "        Computes a best response\n",
    "\n",
    "        Parameters:\n",
    "            s_other : (np.Array) strategy profile of other player\n",
    "\n",
    "        NOTE: this ONLY works with symetric payoffs and 2 actions\n",
    "        '''\n",
    "    \n",
    "        #get EU of action 0\n",
    "    \n",
    "        s = [np.array([1, 0]), s_other]\n",
    "        eu_0 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        # get EU of action 1\n",
    "        s = [np.array([0, 1]), s_other]\n",
    "        eu_1 = expected_utility(s, self.payoffs[0])\n",
    "\n",
    "        return softmax(np.array([eu_0, eu_1])*lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 119.22131505632876\n",
      "     jac: array([-28.  , -28.  , -28.  ,  -0.  ,   3.75,   0.  ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 14\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([3.33333333e-01, 3.33333333e-01, 3.33333333e-01, 1.00000000e+00,\n",
      "       4.83524332e-12, 6.65023592e-14])\n"
     ]
    }
   ],
   "source": [
    "m = Temporal_Level_Model()\n",
    "print(m.fit(data))\n",
    "t1 = data[0][0]\n",
    "t2 = data[0][1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "174d5a967417a2ba15b92fee30b6658756f13acfa6c5a4bc13885d05104c7799"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('bgt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
